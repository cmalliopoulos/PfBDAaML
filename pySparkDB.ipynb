{"cells":[{"cell_type":"markdown","source":["![MLTrain logo](https://mltrain.cc/wp-content/uploads/2017/11/mltrain_logo-4.png \"MLTrain logo\")\n\n----------------\n# Preparations #\nCheck the environment for basic functionality\n\n[Readonly public link](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2554702015877214/632957142081533/8550007600826264/latest.html)"],"metadata":{}},{"cell_type":"code","source":["import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport sklearn as skl\nimport datetime as dt\n\nfrom os import linesep as endl\n\nwith plt.style.context('ggplot'):\n  fig, [ax1, ax2] = plt.subplots(1, 2, figsize = [8, 3])\n  ax1.plot(np.random.randn(100))\n\n  df = pd.DataFrame(np.random.randn(10, 4), columns = ['one', 'two', 'three', 'four'])\n  ax2.plot(df.two)\n  display()\n\n\n"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["# TL;DR #\nHigh-level Spark APIs (DataFrame, Dataset and SparkSql) are made available through a SparkSession cluster-level object.  \nIn the following we'll use a SparkSession to construct Spark DataFrames, register them as tables and execute SparkSQL over these tables.  \n  \n__Databricks__ comes with a SparkSession object instantiated as `spark`"],"metadata":{}},{"cell_type":"code","source":["sses = spark.builder.appName(\"ssapp01\").getOrCreate()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["### Upload a csv file to your Databricks cluster ###  \nIn Databricks we have to upload the .csv file manually:  \n1. From the left of this screen select 'Data' > Table +,  \n2. drag and drop to the rectangle on the right the file appleSales.csv that's contained in PfBDAaML Github repository  \n  \nThe file is stored on your cluster under `/FileStore/tables`  \nWe use SparkSession's csv reader to create a Spark Dataframe object from the csv file:"],"metadata":{}},{"cell_type":"code","source":["df = sses.read.csv('/FileStore/tables/appleStocks.csv', header = True)\nprint type(df)\ndf.show(n = 10, truncate = True)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["__NB:__ There's no type info in the csv file so everything is parsed as string.  \nThis is not good for doing transformations on the file's data.  \nWe ameliorate this by setting up a _schema_ for the file explicitly later on."],"metadata":{}},{"cell_type":"code","source":["print 'df fields and their types:'\ndf.printSchema()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["from pyspark.sql.types import StructField, StringType, IntegerType, DateType, FloatType, StructType\n\ndataSchema = [\n    StructField(\"Date\", DateType(), nullable = True), \n    StructField(\"Open\", FloatType(), True),\n    StructField(\"Close\", FloatType(), True),\n    StructField(\"High\", FloatType(), True),\n    StructField(\"Low\", FloatType(), True),\n    StructField(\"Volume\", IntegerType(), True),\n    StructField(\"Adj Close\", StringType(), True)]\n\ndataSchema = StructType(fields = dataSchema)\ndf = spark.read.csv('/FileStore/tables/appleStocks.csv', schema = dataSchema)\n\ndf.printSchema()\ndf.show(n = 10, truncate = True)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["# Projections and selections #\n\nSlicing rows and columns of pyspark DataFrames is similar to pandas:"],"metadata":{}},{"cell_type":"code","source":["# import pandas as pd\n# df[col.isin(*pd.date_range(dt.datetime(2016, 1, 1), dt.datetime(2016, 2, 1)))].collect()\n\ndf[['Date', 'Open']].show(5)\n\n# Selection in pySpark is a relational algebra projection:\ndf.select(['Open', 'Close']).show(5)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# Select the volumes of stocks exchanged after 01 Jan 2016\ndf.filter(df['Date'] > dt.datetime(2016, 1, 1)).select(['Volume']).show(5)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["# Adding new columns #\n\nWe import `month` ufunc from `pyspark.sql.functions` (a great deal of ufuncs are defined there).  \nThen create a binned column 'Month' to use it for our aggregations in the sequel"],"metadata":{}},{"cell_type":"code","source":["import pyspark.sql.functions as pf\n\ndf = df.withColumn('Month', pf.month(df.Date))\ndf.select('Date', 'Month').sample(False, 0.1).show(5)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["----------------------------------------------\n# Grouping and aggregating #\n\nAs in Pandas, the groupby method returns a `groupby` object which we can aggregate on:"],"metadata":{}},{"cell_type":"code","source":["applesGroup = df.groupby('Month')"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["applesGroup.mean('Open').show(2)\napplesGroup.max('Open').show(2)\napplesGroup.count().show(2)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["### Simultaneous aggregations ###"],"metadata":{}},{"cell_type":"code","source":["applesGroup.agg({'High': 'min', 'Low': 'max', 'Open': 'mean'}).show(5)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["# OrderBy method #"],"metadata":{}},{"cell_type":"code","source":["df.orderBy(\"Month\").show(5)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# More complex orderings are possible\ndf.orderBy(pf.year(df.Date).desc()).show(5)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["## Functions\nThere are a variety of functions you can import from pyspark.sql.functions.  \nCheck out the [Spark documentation](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions) for the full list available"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import countDistinct, avg, stddev, year\n\ndf.select(countDistinct(year(df.Date))).show()\ndf.select(avg('Open')).show()\ndf.select(stddev('Low')).show()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["-----------------------\n# Exporting to pandas #"],"metadata":{}},{"cell_type":"code","source":["import datetime as dt\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.set_option('display.width', 144)\n\npandf = df.select('Date', 'High', 'Low', 'Open', 'Close').toPandas()\npandf['Date'] = pd.to_datetime(pandf['Date'], errors = 'raise')\npandf = pandf.sort_values(axis = 'index', by = 'Date')\n\nfor col in pandf.columns[1:]:\n    pandf[col] = pandf[col].astype(float)\n\n# Normalize\npandf.High = pandf.High.map(lambda _: np.log(_ + 1.) * 1.05)\npandf.Low = pandf.Low.map(lambda _: np.log(_ + 1.) * .95)\npandf[['Open', 'Close']] = pandf[['Open', 'Close']].apply(lambda _: np.log(_ + 1.))\n\nprint pandf.dtypes\n# print np.sort(pandf.Open.unique())[:10]\n\npandfs = pandf['Open'].sample(100, random_state = 101)\n\nax = plt.figure().add_subplot('111')\nax.fill_between(x = pandf.Date.values, y1 = pandf.High.values, y2 = pandf.Low.values, alpha = .3)\nax.plot(pandf.Date.values, pandf.Open.values, pandf.Date.values, pandf.Close.values - .1)\ndisplay()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":27}],"metadata":{"name":"pySpark","notebookId":632957142081533},"nbformat":4,"nbformat_minor":0}
